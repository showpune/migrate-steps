diff --git a/Biblivre/pom.xml b/Biblivre/pom.xml
index 17877aaf4..ffc591881 100644
--- a/Biblivre/pom.xml
+++ b/Biblivre/pom.xml
@@ -155,18 +155,14 @@
             <version>1.2.1</version>
         </dependency>
         <dependency>
-            <groupId>software.amazon.awssdk</groupId>
-            <artifactId>apache-client</artifactId>
+            <groupId>com.azure</groupId>
+            <artifactId>azure-storage-blob</artifactId>
+            <version>12.14.2</version>
         </dependency>
         <dependency>
-            <groupId>software.amazon.awssdk</groupId>
-            <artifactId>s3</artifactId>
-            <exclusions>
-                <exclusion>
-                    <groupId>software.amazon.awssdk</groupId>
-                    <artifactId>netty-nio-client</artifactId>
-                </exclusion>
-            </exclusions>
+            <groupId>com.azure</groupId>
+            <artifactId>azure-identity</artifactId>
+            <version>1.5.3</version>
         </dependency>
         <dependency>
             <groupId>org.springframework</groupId>
diff --git a/Biblivre/src/main/java/biblivre/digitalmedia/migrator/PGToS3DigitalMediaMigrator.java b/Biblivre/src/main/java/biblivre/digitalmedia/migrator/PGToS3DigitalMediaMigrator.java
index 00dc43159..b25fae9c8 100644
--- a/Biblivre/src/main/java/biblivre/digitalmedia/migrator/PGToS3DigitalMediaMigrator.java
+++ b/Biblivre/src/main/java/biblivre/digitalmedia/migrator/PGToS3DigitalMediaMigrator.java
@@ -7,6 +7,11 @@ import biblivre.core.schemas.SchemaDTO;
 import biblivre.digitalmedia.DigitalMediaDTO;
 import biblivre.digitalmedia.postgres.DatabaseFile;
 import biblivre.digitalmedia.postgres.PostgresLargeObjectDigitalMediaDAO;
+import com.azure.identity.DefaultAzureCredentialBuilder;
+import com.azure.storage.blob.BlobClient;
+import com.azure.storage.blob.BlobClientBuilder;
+import com.azure.storage.blob.BlobContainerClient;
+import com.azure.storage.blob.BlobContainerClientBuilder;
 import java.io.InputStream;
 import java.sql.Connection;
 import java.sql.SQLException;
@@ -20,27 +25,26 @@ import org.postgresql.largeobject.LargeObjectManager;
 import org.springframework.beans.factory.annotation.Autowired;
 import org.springframework.boot.autoconfigure.condition.ConditionalOnProperty;
 import org.springframework.stereotype.Component;
-import software.amazon.awssdk.auth.credentials.DefaultCredentialsProvider;
-import software.amazon.awssdk.core.sync.RequestBody;
-import software.amazon.awssdk.services.s3.S3Client;
-import software.amazon.awssdk.services.s3.model.PutObjectRequest;
 
 @Component
 @ConditionalOnProperty(name = "DIGITAL_MEDIA_MIGRATOR")
 @Slf4j
 public class PGToS3DigitalMediaMigrator implements DigitalMediaStoreMigrator {
-    private S3Client s3;
-    private String bucketName;
+    private BlobContainerClient blobContainerClient;
+    private String containerName;
     private SchemaBO schemaBO;
 
     private PostgresLargeObjectDigitalMediaDAO digitalMediaDAO;
 
     @Override
     public void init() {
-        this.s3 =
-                S3Client.builder().credentialsProvider(DefaultCredentialsProvider.create()).build();
+        this.blobContainerClient = new BlobContainerClientBuilder()
+                .credential(new DefaultAzureCredentialBuilder().build())
+                .endpoint(System.getenv("AZURE_STORAGE_ENDPOINT"))
+                .containerName(System.getenv("AZURE_STORAGE_CONTAINER_NAME"))
+                .buildClient();
 
-        this.bucketName = System.getenv("S3_BUCKET_NAME");
+        this.containerName = System.getenv("AZURE_STORAGE_CONTAINER_NAME");
     }
 
     @Override
@@ -50,7 +54,7 @@ public class PGToS3DigitalMediaMigrator implements DigitalMediaStoreMigrator {
 
     @Override
     public String to() {
-        return "AWS S3";
+        return "Azure Storage Account";
     }
 
     @Override
@@ -91,7 +95,7 @@ public class PGToS3DigitalMediaMigrator implements DigitalMediaStoreMigrator {
                                     media.getName(),
                                     FileUtils.byteCountToDisplaySize(databaseFile.getSize()));
 
-                            _uploadToS3(databaseFile);
+                            _uploadToAzure(databaseFile);
 
                             log.info("Removing {}, (id: {})", media.getName(), media.getId());
 
@@ -125,22 +129,16 @@ public class PGToS3DigitalMediaMigrator implements DigitalMediaStoreMigrator {
         largeObjectAPI.delete(oid);
     }
 
-    private void _uploadToS3(DatabaseFile databaseFile) throws SQLException {
+    private void _uploadToAzure(DatabaseFile databaseFile) throws SQLException {
         LargeObject largeObject = databaseFile.getLargeObject();
 
         long oid = largeObject.getLongOID();
 
-        PutObjectRequest request =
-                PutObjectRequest.builder()
-                        .bucket(bucketName)
-                        .key(String.valueOf(oid))
-                        .contentType(databaseFile.getContentType())
-                        .contentLength(databaseFile.getSize())
-                        .build();
+        BlobClient blobClient = blobContainerClient.getBlobClient(String.valueOf(oid));
 
         InputStream inputStream = largeObject.getInputStream();
 
-        s3.putObject(request, RequestBody.fromInputStream(inputStream, databaseFile.getSize()));
+        blobClient.upload(inputStream, databaseFile.getSize(), true);
     }
 
     @Autowired
diff --git a/Biblivre/src/main/java/biblivre/digitalmedia/s3/S3DigitalMediaDAO.java b/Biblivre/src/main/java/biblivre/digitalmedia/s3/S3DigitalMediaDAO.java
index 03a3153d6..4f7d53a44 100644
--- a/Biblivre/src/main/java/biblivre/digitalmedia/s3/S3DigitalMediaDAO.java
+++ b/Biblivre/src/main/java/biblivre/digitalmedia/s3/S3DigitalMediaDAO.java
@@ -9,64 +9,54 @@ import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 import org.springframework.boot.autoconfigure.condition.ConditionalOnProperty;
 import org.springframework.stereotype.Service;
-import software.amazon.awssdk.auth.credentials.DefaultCredentialsProvider;
-import software.amazon.awssdk.core.ResponseInputStream;
-import software.amazon.awssdk.core.sync.RequestBody;
-import software.amazon.awssdk.services.s3.S3Client;
-import software.amazon.awssdk.services.s3.model.DeleteObjectRequest;
-import software.amazon.awssdk.services.s3.model.GetObjectRequest;
-import software.amazon.awssdk.services.s3.model.GetObjectResponse;
-import software.amazon.awssdk.services.s3.model.PutObjectRequest;
+import com.azure.storage.blob.BlobClient;
+import com.azure.storage.blob.BlobClientBuilder;
+import com.azure.storage.blob.models.BlobHttpHeaders;
+import com.azure.storage.blob.models.BlobProperties;
 
 @Service
 @ConditionalOnProperty(
         value = "DIGITAL_MEDIA_DAO_IMPL",
         havingValue = "biblivre.digitalmedia.s3.S3DigitalMediaDAO")
 public class S3DigitalMediaDAO extends BaseDigitalMediaDAO {
-    private final S3Client s3;
-    private final String bucketName;
+    private final BlobClient blobClient;
+    private final String containerName;
 
     private static final Logger logger = LoggerFactory.getLogger(S3DigitalMediaDAO.class);
 
     public S3DigitalMediaDAO() {
-        this.s3 =
-                S3Client.builder().credentialsProvider(DefaultCredentialsProvider.create()).build();
-
-        this.bucketName = System.getenv("S3_BUCKET_NAME");
+        this.containerName = System.getenv("AZURE_STORAGE_CONTAINER_NAME");
+        String connectionString = System.getenv("AZURE_STORAGE_CONNECTION_STRING");
+        this.blobClient = new BlobClientBuilder()
+                .connectionString(connectionString)
+                .containerName(containerName)
+                .buildClient();
     }
 
     @Override
     protected void persist(InputStream is, long oid, long size) {
 
         logger.info(
-                "Uploading file to S3, (oid: {}, size: {})",
+                "Uploading file to Azure Blob Storage, (oid: {}, size: {})",
                 oid,
                 FileUtils.byteCountToDisplaySize(size));
 
-        PutObjectRequest request =
-                PutObjectRequest.builder()
-                        .bucket(bucketName)
-                        .key(String.valueOf(oid))
-                        .metadata(Collections.singletonMap("oid", String.valueOf(oid)))
-                        .build();
-
-        s3.putObject(request, RequestBody.fromInputStream(is, size));
+        BlobClient client = blobClient.getBlobClient(String.valueOf(oid));
+        client.upload(is, size, true);
+        client.setHttpHeaders(new BlobHttpHeaders().setMetadata(Collections.singletonMap("oid", String.valueOf(oid))));
     }
 
     @Override
     protected void deleteBlob(long oid) {
-        DeleteObjectRequest request =
-                DeleteObjectRequest.builder().bucket(bucketName).key(String.valueOf(oid)).build();
-
-        s3.deleteObject(request);
+        BlobClient client = blobClient.getBlobClient(String.valueOf(oid));
+        client.delete();
     }
 
     @Override
     protected BiblivreFile getFile(long oid) {
-        GetObjectRequest request =
-                GetObjectRequest.builder().bucket(bucketName).key(String.valueOf(oid)).build();
-
-        ResponseInputStream<GetObjectResponse> inputStream = s3.getObject(request);
+        BlobClient client = blobClient.getBlobClient(String.valueOf(oid));
+        BlobProperties properties = client.getProperties();
+        InputStream inputStream = client.openInputStream();
 
         return new S3File(inputStream);
     }
