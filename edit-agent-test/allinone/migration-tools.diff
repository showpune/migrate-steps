diff --git a/Biblivre/src/main/java/biblivre/digitalmedia/migrator/PGToS3DigitalMediaMigrator.java b/Biblivre/src/main/java/biblivre/digitalmedia/migrator/PGToS3DigitalMediaMigrator.java
index 00dc43159..213f09450 100644
--- a/Biblivre/src/main/java/biblivre/digitalmedia/migrator/PGToS3DigitalMediaMigrator.java
+++ b/Biblivre/src/main/java/biblivre/digitalmedia/migrator/PGToS3DigitalMediaMigrator.java
@@ -7,6 +7,10 @@ import biblivre.core.schemas.SchemaDTO;
 import biblivre.digitalmedia.DigitalMediaDTO;
 import biblivre.digitalmedia.postgres.DatabaseFile;
 import biblivre.digitalmedia.postgres.PostgresLargeObjectDigitalMediaDAO;
+import com.azure.identity.DefaultAzureCredentialBuilder;
+import com.azure.storage.blob.BlobServiceClient;
+import com.azure.storage.blob.BlobServiceClientBuilder;
+import com.azure.storage.blob.models.BlobHttpHeaders;
 import java.io.InputStream;
 import java.sql.Connection;
 import java.sql.SQLException;
@@ -20,27 +24,25 @@ import org.postgresql.largeobject.LargeObjectManager;
 import org.springframework.beans.factory.annotation.Autowired;
 import org.springframework.boot.autoconfigure.condition.ConditionalOnProperty;
 import org.springframework.stereotype.Component;
-import software.amazon.awssdk.auth.credentials.DefaultCredentialsProvider;
-import software.amazon.awssdk.core.sync.RequestBody;
-import software.amazon.awssdk.services.s3.S3Client;
-import software.amazon.awssdk.services.s3.model.PutObjectRequest;
 
 @Component
 @ConditionalOnProperty(name = "DIGITAL_MEDIA_MIGRATOR")
 @Slf4j
 public class PGToS3DigitalMediaMigrator implements DigitalMediaStoreMigrator {
-    private S3Client s3;
-    private String bucketName;
+    private BlobServiceClient blobServiceClient;
+    private String containerName;
     private SchemaBO schemaBO;
 
     private PostgresLargeObjectDigitalMediaDAO digitalMediaDAO;
 
     @Override
     public void init() {
-        this.s3 =
-                S3Client.builder().credentialsProvider(DefaultCredentialsProvider.create()).build();
+        this.blobServiceClient = new BlobServiceClientBuilder()
+                .credential(new DefaultAzureCredentialBuilder().build())
+                .endpoint(System.getenv("AZURE_STORAGE_BLOB_ENDPOINT"))
+                .buildClient();
 
-        this.bucketName = System.getenv("S3_BUCKET_NAME");
+        this.containerName = System.getenv("AZURE_STORAGE_CONTAINER_NAME");
     }
 
     @Override
@@ -50,7 +52,7 @@ public class PGToS3DigitalMediaMigrator implements DigitalMediaStoreMigrator {
 
     @Override
     public String to() {
-        return "AWS S3";
+        return "Azure Blob Storage";
     }
 
     @Override
@@ -91,7 +93,7 @@ public class PGToS3DigitalMediaMigrator implements DigitalMediaStoreMigrator {
                                     media.getName(),
                                     FileUtils.byteCountToDisplaySize(databaseFile.getSize()));
 
-                            _uploadToS3(databaseFile);
+                            _uploadToBlob(databaseFile);
 
                             log.info("Removing {}, (id: {})", media.getName(), media.getId());
 
@@ -125,22 +127,19 @@ public class PGToS3DigitalMediaMigrator implements DigitalMediaStoreMigrator {
         largeObjectAPI.delete(oid);
     }
 
-    private void _uploadToS3(DatabaseFile databaseFile) throws SQLException {
+    private void _uploadToBlob(DatabaseFile databaseFile) throws SQLException {
         LargeObject largeObject = databaseFile.getLargeObject();
 
         long oid = largeObject.getLongOID();
 
-        PutObjectRequest request =
-                PutObjectRequest.builder()
-                        .bucket(bucketName)
-                        .key(String.valueOf(oid))
-                        .contentType(databaseFile.getContentType())
-                        .contentLength(databaseFile.getSize())
-                        .build();
+        BlobHttpHeaders headers = new BlobHttpHeaders()
+                .setContentType(databaseFile.getContentType());
 
         InputStream inputStream = largeObject.getInputStream();
 
-        s3.putObject(request, RequestBody.fromInputStream(inputStream, databaseFile.getSize()));
+        blobServiceClient.getBlobContainerClient(containerName)
+                .getBlobClient(String.valueOf(oid))
+                .upload(inputStream, databaseFile.getSize(), true);
     }
 
     @Autowired
@@ -152,4 +151,4 @@ public class PGToS3DigitalMediaMigrator implements DigitalMediaStoreMigrator {
     public void setDigitalMediaDAO(PostgresLargeObjectDigitalMediaDAO digitalMediaDAO) {
         this.digitalMediaDAO = digitalMediaDAO;
     }
-}
+}
\ No newline at end of file
diff --git a/Biblivre/src/main/java/biblivre/digitalmedia/s3/S3DigitalMediaDAO.java b/Biblivre/src/main/java/biblivre/digitalmedia/s3/S3DigitalMediaDAO.java
index 03a3153d6..244f5bcae 100644
--- a/Biblivre/src/main/java/biblivre/digitalmedia/s3/S3DigitalMediaDAO.java
+++ b/Biblivre/src/main/java/biblivre/digitalmedia/s3/S3DigitalMediaDAO.java
@@ -2,72 +2,75 @@ package biblivre.digitalmedia.s3;
 
 import biblivre.core.file.BiblivreFile;
 import biblivre.digitalmedia.BaseDigitalMediaDAO;
-import java.io.InputStream;
-import java.util.Collections;
+import com.azure.identity.DefaultAzureCredentialBuilder;
+import com.azure.storage.blob.BlobServiceClient;
+import com.azure.storage.blob.BlobServiceClientBuilder;
+import com.azure.storage.blob.models.BlobHttpHeaders;
+import com.azure.storage.blob.models.BlobProperties;
+import com.azure.storage.blob.models.BlobRange;
+import com.azure.storage.blob.options.BlobParallelUploadOptions;
 import org.apache.commons.io.FileUtils;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 import org.springframework.boot.autoconfigure.condition.ConditionalOnProperty;
 import org.springframework.stereotype.Service;
-import software.amazon.awssdk.auth.credentials.DefaultCredentialsProvider;
-import software.amazon.awssdk.core.ResponseInputStream;
-import software.amazon.awssdk.core.sync.RequestBody;
-import software.amazon.awssdk.services.s3.S3Client;
-import software.amazon.awssdk.services.s3.model.DeleteObjectRequest;
-import software.amazon.awssdk.services.s3.model.GetObjectRequest;
-import software.amazon.awssdk.services.s3.model.GetObjectResponse;
-import software.amazon.awssdk.services.s3.model.PutObjectRequest;
+
+import java.io.InputStream;
+import java.util.Collections;
 
 @Service
 @ConditionalOnProperty(
         value = "DIGITAL_MEDIA_DAO_IMPL",
         havingValue = "biblivre.digitalmedia.s3.S3DigitalMediaDAO")
 public class S3DigitalMediaDAO extends BaseDigitalMediaDAO {
-    private final S3Client s3;
-    private final String bucketName;
+    private final BlobServiceClient blobServiceClient;
+    private final String containerName;
 
     private static final Logger logger = LoggerFactory.getLogger(S3DigitalMediaDAO.class);
 
     public S3DigitalMediaDAO() {
-        this.s3 =
-                S3Client.builder().credentialsProvider(DefaultCredentialsProvider.create()).build();
+        this.blobServiceClient = new BlobServiceClientBuilder()
+                .credential(new DefaultAzureCredentialBuilder().build())
+                .endpoint(System.getenv("AZURE_STORAGE_BLOB_ENDPOINT"))
+                .buildClient();
 
-        this.bucketName = System.getenv("S3_BUCKET_NAME");
+        this.containerName = System.getenv("AZURE_STORAGE_CONTAINER_NAME");
     }
 
     @Override
     protected void persist(InputStream is, long oid, long size) {
 
         logger.info(
-                "Uploading file to S3, (oid: {}, size: {})",
+                "Uploading file to Azure Blob Storage, (oid: {}, size: {})",
                 oid,
                 FileUtils.byteCountToDisplaySize(size));
 
-        PutObjectRequest request =
-                PutObjectRequest.builder()
-                        .bucket(bucketName)
-                        .key(String.valueOf(oid))
-                        .metadata(Collections.singletonMap("oid", String.valueOf(oid)))
-                        .build();
+        BlobParallelUploadOptions options = new BlobParallelUploadOptions(is, size)
+                .setHeaders(new BlobHttpHeaders().setContentType("application/octet-stream"))
+                .setMetadata(Collections.singletonMap("oid", String.valueOf(oid)));
 
-        s3.putObject(request, RequestBody.fromInputStream(is, size));
+        blobServiceClient.getBlobContainerClient(containerName)
+                .getBlobClient(String.valueOf(oid))
+                .uploadWithResponse(options, null, null);
     }
 
     @Override
     protected void deleteBlob(long oid) {
-        DeleteObjectRequest request =
-                DeleteObjectRequest.builder().bucket(bucketName).key(String.valueOf(oid)).build();
-
-        s3.deleteObject(request);
+        blobServiceClient.getBlobContainerClient(containerName)
+                .getBlobClient(String.valueOf(oid))
+                .delete();
     }
 
     @Override
     protected BiblivreFile getFile(long oid) {
-        GetObjectRequest request =
-                GetObjectRequest.builder().bucket(bucketName).key(String.valueOf(oid)).build();
+        BlobProperties properties = blobServiceClient.getBlobContainerClient(containerName)
+                .getBlobClient(String.valueOf(oid))
+                .getProperties();
 
-        ResponseInputStream<GetObjectResponse> inputStream = s3.getObject(request);
+        InputStream inputStream = blobServiceClient.getBlobContainerClient(containerName)
+                .getBlobClient(String.valueOf(oid))
+                .openQueryInputStream(new BlobRange(0, properties.getBlobSize()));
 
         return new S3File(inputStream);
     }
-}
+}
\ No newline at end of file
